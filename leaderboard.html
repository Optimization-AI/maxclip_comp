<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MaxCLIP Competition</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">

</head>
<body class="antialiased leading-normal tracking-normal text-gray-300">
    <!-- background blobs -->
    <div class="blob"></div>
    <div class="blob"></div>
    <div class="blob"></div>

    <nav class="navbar animate__animated animate__fadeInDown">
        <div class="container mx-auto px-4 py-2 flex justify-between items-center">
            <a class="brand-text text-2xl font-bold text-white-custom flex items-center" href="index.html">
                <img src="images/logo_transparent.png" alt="" class="h-10 mr-2"> MaxCLIP
            </a>        
            <div class="hidden lg:flex space-x-4">
                <a class="navbar-link" href="index.html">Home</a>
                <a class="navbar-link" href="competition.html">Competition</a>
                <a class="navbar-link" href="leaderboard.html">Leaderboard</a>
                <a class="navbar-link" href="participate.html">Participate</a>
                <a class="navbar-link" href="about.html">About</a>
                <a class="navbar-link" href="contact.html">Contact</a>
            </div>
            <button class="lg:hidden" id="navbarToggle">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7" />
                </svg>
            </button>
        </div>
        <div id="mobileMenu" class="lg:hidden flex flex-col mt-4 space-y-2">
            <a class="mobile-menu-item" href="index.html">Home</a>
            <a class="mobile-menu-item" href="competition.html">Competition</a>
            <a class="mobile-menu-item" href="leaderboard.html">Leaderboard</a>
            <a class="mobile-menu-item" href="participate.html">Participate</a>
            <a class="mobile-menu-item" href="about.html">About</a>
            <a class="mobile-menu-item" href="contact.html">Contact</a>
        </div>
    </nav>
    

    <div class="container mx-auto px-4 py-6 animate__animated animate__fadeInUp">
        <div class="bg-white p-6 rounded-lg shadow-lg">
            <h1 class="text-3xl font-bold mb-4 text-center text-black">Leaderboard</h1>
            <p class="text-lg mb-6 text-center text-black">
                Below is the leaderboard for our competition, including our baseline <a href="https://arxiv.org/abs/2407.01445" class="text-blue-500 hover:underline">FastCLIP</a>.
            </p>
    
            <!-- Tabs for Constrained and Unconstrained -->
            <center>
                <div class="mb-4">
                    <button id="unconstrainedTab" class="px-4 py-2 bg-blue-500 text-white rounded" onclick="showTable('unconstrained')">Unconstrained</button>
                    <button id="constrainedTab" class="px-4 py-2 bg-gray-300 text-black rounded ml-2" onclick="showTable('constrained')">Constrained</button>
                </div>
            </center>

            <!-- Constrained Table -->
            <div id="constrainedTable" class="overflow-x-auto hidden">
                <center>
                    <div class="mb-4">
                        <button id="smallTab" class="px-4 py-2 bg-blue-500 text-white rounded ml-2" onclick="showSubTable('small')">Small</button>
                        <button id="mediumTab" class="px-4 py-2 bg-gray-300 text-black rounded ml-2" onclick="showSubTable('medium')">Medium</button>
                        <button id="largeTab" class="px-4 py-2 bg-gray-300 text-black rounded ml-2" onclick="showSubTable('large')">Large</button>
                    </div>
                </center>

                <!-- Small Table -->
                <div id="smallTable" class="overflow-x-auto">
                    <table id="smallLeaderboardTable" class="display w-full text-left text-black">
                        <thead>
                            <tr>
                                <th>Name</th>
                                <th>DataComp Average</th>
                                <th>ImageNet Zero-Shot Accuracy</th>
                                <th>Organization</th>
                                <th>Submission Date</th>
                                <th>Writeup</th>
                            </tr>
                        </thead>
                        <tbody>
                        </tbody>
                    </table>
                </div>

                <!-- Medium Table -->
                <div id="mediumTable" class="overflow-x-auto hidden">
                    <table id="mediumLeaderboardTable" class="display w-full text-left text-black">
                        <thead>
                            <tr>
                                <th>Name</th>
                                <th>DataComp Average</th>
                                <th>ImageNet Zero-Shot Accuracy</th>
                                <th>Organization</th>
                                <th>Submission Date</th>
                                <th>Writeup</th>
                            </tr>
                        </thead>
                        <tbody>
                        </tbody>
                    </table>
                </div>

                <!-- Large Table -->
                <div id="largeTable" class="overflow-x-auto hidden">
                    <table id="largeLeaderboardTable" class="display w-full text-left text-black">
                        <thead>
                            <tr>
                                <th>Name</th>
                                <th>DataComp Average</th>
                                <th>ImageNet Zero-Shot Accuracy</th>
                                <th>Organization</th>
                                <th>Submission Date</th>
                                <th>Writeup</th>
                            </tr>
                        </thead>
                        <tbody>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- Unconstrained Table -->
            <div id="unconstrainedTable" class="overflow-x-auto">
                <table id="unconstrainedLeaderboardTable" class="display w-full text-left text-black">
                    <thead>
                        <tr>
                            <th>Name</th>
                            <th>Model</th>
                            <th>Dataset</th>
                            <th>Batch Size (k)</th>
                            <th>Samples Seen (B)</th>
                            <th>ImageNet-1K Top 1</th>
                            <th>DataComp Average</th>
                            <th>Weights</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <th><a href="https://openreview.net/pdf?id=ZKtZ7KQ6G5">DFN</a></th>
                            <td>ViT-H/14</td>
                            <td>DFN-5B</td>
                            <td>78</td>
                            <td>44</td>
                            <td>84.35</td>
                            <td>70.79</td>
                            <td><a href="https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/pdf?id=ZKtZ7KQ6G5">DFN</a></th>
                            <td>ViT-H/14</td>
                            <td>DFN-5B</td>
                            <td>78</td>
                            <td>39</td>
                            <td>83.43</td>
                            <td>69.62</td>
                            <td><a href="https://huggingface.co/immich-app/ViT-H-14-quickgelu__dfn5b">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-SO400M/14</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>45</td>
                            <td>83.09</td>
                            <td>69.21</td>
                            <td><a href="https://huggingface.co/timm/ViT-SO400M-14-SigLIP-384">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html">CLIPA</a></th>
                            <td>ViT-bigG/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.4</td>
                            <td>83.07</td>
                            <td>68.44</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-bigG-14-CLIPA-336-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-XL</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>41</td>
                            <td>82.67</td>
                            <td>68.12</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-XL-336px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html">CLIPA</a></th>
                            <td>ViT-bigG/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.3</td>
                            <td>82.67</td>
                            <td>68.26</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-bigG-14-CLIPA-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-XL</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>40</td>
                            <td>82.25</td>
                            <td>67.66</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-XL-256px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-L/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>45</td>
                            <td>82.10</td>
                            <td>66.83</td>
                            <td><a href="https://huggingface.co/timm/ViT-L-16-SigLIP-384">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L2</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13.3</td>
                            <td>82.05</td>
                            <td>67.97</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L2-384px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-SO400M/14</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>40</td>
                            <td>82.04</td>
                            <td>68.07</td>
                            <td><a href="https://huggingface.co/timm/ViT-SO400M-14-SigLIP">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-E/14</td>
                            <td>LAION-2B</td>
                            <td>144</td>
                            <td>9</td>
                            <td>81.97</td>
                            <td>69.30</td>
                            <td><a href="https://huggingface.co/timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-E/14</td>
                            <td>LAION-2B</td>
                            <td>115</td>
                            <td>4</td>
                            <td>81.96</td>
                            <td>66.66</td>
                            <td><a href="https://huggingface.co/timm/eva02_enormous_patch14_clip_224.laion2b_s4b_b115k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13</td>
                            <td>81.83</td>
                            <td>67.12</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L-384px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html">CLIPA</a></th>
                            <td>ViT-H/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.4</td>
                            <td>81.81</td>
                            <td>66.77</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-H-14-CLIPA-336-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L2</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13.3</td>
                            <td>81.80</td>
                            <td>67.72</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L2-336px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13</td>
                            <td>81.60</td>
                            <td>66.88</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L-336px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html">CLIPA</a></th>
                            <td>ViT-H/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.3</td>
                            <td>81.50</td>
                            <td>66.53</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-H-14-CLIPA-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L2</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13.3</td>
                            <td>81.43</td>
                            <td>67.20</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L2-256px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/pdf?id=ZKtZ7KQ6G5">DFN</a></th>
                            <td>ViT-L/14</td>
                            <td>DFN-2B</td>
                            <td>88</td>
                            <td>12.8</td>
                            <td>81.41</td>
                            <td>66.65</td>
                            <td><a href="https://huggingface.co/apple/DFN2B-CLIP-ViT-L-14">Huggingface</a></td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>13</td>
                            <td>81.19</td>
                            <td>66.91</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L-256px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L2</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>12.8</td>
                            <td>80.89</td>
                            <td>67.39</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L2-224px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html">ViTamin</a></th>
                            <td>ViTamin-L</td>
                            <td>Datacomp-1B</td>
                            <td>90</td>
                            <td>12.8</td>
                            <td>80.77</td>
                            <td>66.59</td>
                            <td><a href="https://huggingface.co/jienengchen/ViTamin-L-224px">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=5BCFlnfE1g">MetaCLIP</a></th>
                            <td>ViT-H/14</td>
                            <td>MetaCLIP-2.5B</td>
                            <td>32</td>
                            <td>12.8</td>
                            <td>80.53</td>
                            <td>66.72</td>
                            <td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/h14_fullcc2.5b.pt">Meta</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-L/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>40</td>
                            <td>80.43</td>
                            <td>65.56</td>
                            <td><a href="https://huggingface.co/timm/ViT-L-16-SigLIP-256">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-L/14</td>
                            <td>Merged-2B (1.6B from LAION-2B, 0.4B from COYO-700M)</td>
                            <td>61</td>
                            <td>6</td>
                            <td>80.37</td>
                            <td>65.61</td>
                            <td><a href="https://huggingface.co/timm/eva02_large_patch14_clip_336.merged2b_s6b_b61k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2306.15658">CLIPA-v2</a></th>
                            <td>ViT-L/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.4</td>
                            <td>80.27</td>
                            <td>65.69</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-L-14-CLIPA-336-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-bigG/14</td>
                            <td>LAION-2B</td>
                            <td>160</td>
                            <td>36</td>
                            <td>80.08</td>
                            <td>66.65</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-L/14</td>
                            <td>Merged-2B (1.6B from LAION-2B, 0.4B from COYO-700M)</td>
                            <td>131</td>
                            <td>4</td>
                            <td>79.78</td>
                            <td>64.76</td>
                            <td><a href="https://huggingface.co/timm/eva02_large_patch14_clip_224.merged2b_s4b_b131k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2306.15658">CLIPA-v2</a></th>
                            <td>ViT-L/14</td>
                            <td>Datacomp-1B</td>
                            <td>64</td>
                            <td>13.3</td>
                            <td>79.60</td>
                            <td>65.35</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-L-14-CLIPA-336-datacomp1B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-XXL</td>
                            <td>LAION-2B</td>
                            <td>82</td>
                            <td>34</td>
                            <td>79.46</td>
                            <td>65.11</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-g/14</td>
                            <td>Merged-2B (1.6B from LAION-2B, 0.4B from COYO-700M)</td>
                            <td>114</td>
                            <td>11</td>
                            <td>79.32</td>
                            <td>66.05</td>
                            <td><a href="https://huggingface.co/timm/eva_giant_patch14_plus_clip_224.merged2b_s11b_b114k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-XXL</td>
                            <td>LAION-2B</td>
                            <td>82</td>
                            <td>34</td>
                            <td>79.32</td>
                            <td>64.96</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=dVaWCDMBof">DataComp</a></th>
                            <td>ViT-L/14</td>
                            <td>CommonPool</td>
                            <td>90</td>
                            <td>13</td>
                            <td>79.21</td>
                            <td>66.28</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=5BCFlnfE1g">MetaCLIP</a></th>
                            <td>ViT-L/14</td>
                            <td>MetaCLIP-2.5B</td>
                            <td>32</td>
                            <td>12.8</td>
                            <td>79.17</td>
                            <td>65.92</td>
                            <td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_fullcc2.5b.pt">Meta</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>45</td>
                            <td>79.13</td>
                            <td>64.59</td>
                            <td><a href="https://huggingface.co/timm/ViT-B-16-SigLIP-512">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html">CLIPA</a></th>
                            <td>ViT-H/14</td>
                            <td>LAION-2B</td>
                            <td>64</td>
                            <td>13.4</td>
                            <td>79.10</td>
                            <td>64.39</td>
                            <td><a href="https://huggingface.co/UCSC-VLAA/ViT-H-14-CLIPA-336-laion2B">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-XXL</td>
                            <td>LAION-2B</td>
                            <td>82</td>
                            <td>34</td>
                            <td>79.07</td>
                            <td>64.93</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://arxiv.org/abs/2303.15389">EVA-CLIP</a></th>
                            <td>ViT-g/14</td>
                            <td>LAION-400M</td>
                            <td>41</td>
                            <td>11</td>
                            <td>78.52</td>
                            <td>63.39</td>
                            <td><a href="https://huggingface.co/timm/eva_giant_patch14_clip_224.laion400m_s11b_b41k">Huggingface</a></td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>45</td>
                            <td>78.48</td>
                            <td>63.79</td>
                            <td><a href="https://huggingface.co/timm/ViT-B-16-SigLIP-384">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-g/14</td>
                            <td>LAION-2B</td>
                            <td>86.7</td>
                            <td>34.5</td>
                            <td>78.48</td>
                            <td>64.26</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s34B-b88K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-H/14</td>
                            <td>LAION-2B</td>
                            <td>77.25</td>
                            <td>34</td>
                            <td>77.92</td>
                            <td>64.02</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.html">MobileCLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>DatacompDR</td>
                            <td>64</td>
                            <td>13</td>
                            <td>77.18</td>
                            <td>65.22</td>
                            <td><a href="https://huggingface.co/apple/MobileCLIP-B-LT-OpenCLIP">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-H/14</td>
                            <td>LAION-5B</td>
                            <td>90</td>
                            <td>13</td>
                            <td>76.97</td>
                            <td>65.15</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-L</td>
                            <td>LAION-2B</td>
                            <td>131</td>
                            <td>29</td>
                            <td>76.83</td>
                            <td>63.87</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Vasu_MobileCLIP_Fast_Image-Text_Models_through_Multi-Modal_Reinforced_Training_CVPR_2024_paper.html">MobileCLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>DatacompDR</td>
                            <td>64</td>
                            <td>13</td>
                            <td>76.83</td>
                            <td>65.03</td>
                            <td><a href="https://huggingface.co/apple/MobileCLIP-B-OpenCLIP">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-g/14</td>
                            <td>LAION-2B</td>
                            <td>42</td>
                            <td>13</td>
                            <td>76.66</td>
                            <td>62.87</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-L</td>
                            <td>LAION-2B</td>
                            <td>131</td>
                            <td>29</td>
                            <td>76.60</td>
                            <td>63.45</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.mlr.press/v139/radford21a">OpenAI CLIP</a></th>
                            <td>ViT-L/14</td>
                            <td>WIT</td>
                            <td>32</td>
                            <td>13.2</td>
                            <td>76.56</td>
                            <td>62.76</td>
                            <td><a href="https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt">OpenAI</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>40</td>
                            <td>76.53</td>
                            <td>62.26</td>
                            <td><a href="https://huggingface.co/timm/ViT-B-16-SigLIP-256">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=dVaWCDMBof">DataComp</a></th>
                            <td>ViT-L/14</td>
                            <td>CommonPool</td>
                            <td>90</td>
                            <td>13</td>
                            <td>76.39</td>
                            <td>64.80</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.clip-s13B-b90K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/pdf?id=ZKtZ7KQ6G5">DFN</a></th>
                            <td>ViT-B/16</td>
                            <td>DFN-2B</td>
                            <td>-</td>
                            <td>12.8</td>
                            <td>76.23</td>
                            <td>60.75</td>
                            <td><a href="https://huggingface.co/apple/DFN2B-CLIP-ViT-B-16">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=5BCFlnfE1g">MetaCLIP</a></th>
                            <td>ViT-L/14</td>
                            <td>MetaCLIP-400M</td>
                            <td>32</td>
                            <td>12.8</td>
                            <td>76.17</td>
                            <td>62.55</td>
                            <td><a href="https://dl.fbaipublicfiles.com/MMPT/metaclip/l14_400m.pt">Meta</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>40</td>
                            <td>76.04</td>
                            <td>62.33</td>
                            <td><a href="https://huggingface.co/timm/ViT-B-16-SigLIP">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ConvNeXt-L</td>
                            <td>LAION-2B</td>
                            <td>102</td>
                            <td>26</td>
                            <td>75.92</td>
                            <td>62.95</td>
                            <td><a href="https://huggingface.co/laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=Ee277P3AYC">CoCa (OpenCLIP)</a></th>
                            <td>ViT-L/14</td>
                            <td>LAION-2B</td>
                            <td>90</td>
                            <td>13</td>
                            <td>75.64</td>
                            <td>62.88</td>
                            <td><a href="https://huggingface.co/laion/CoCa-ViT-L-14-laion2B-s13B-b90k">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://proceedings.mlr.press/v139/radford21a">OpenAI CLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WIT</td>
                            <td>32</td>
                            <td>12.8</td>
                            <td>75.56</td>
                            <td>61.62</td>
                            <td><a href="https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt">OpenAI</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openreview.net/forum?id=dVaWCDMBof">DataComp</a></th>
                            <td>ViT-B/16</td>
                            <td>CommonPool (LAION-2B filtering)</td>
                            <td>90</td>
                            <td>13</td>
                            <td>75.49</td>
                            <td>63.56</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-L-14-CommonPool.XL.laion-s13B-b90K">Huggingface</a></td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper">OpenCLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>LAION-2B</td>
                            <td>82</td>
                            <td>32</td>
                            <td>75.23</td>
                            <td>61.89</td>
                            <td><a href="https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K">Huggingface</a> </td>
                        </tr>
                        <tr>
                            <th><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">SigLIP</a></th>
                            <td>ViT-B/16</td>
                            <td>WebLI</td>
                            <td>32</td>
                            <td>40</td>
                            <td>75.11</td>
                            <td>60.68</td>
                            <td><a href="https://huggingface.co/timm/ViT-B-16-SigLIP-i18n-256">Huggingface</a></td>
                        </tr>
                    </tbody>
                </table>
            </div>
    </div>
    
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.datatables.net/1.13.3/js/jquery.dataTables.min.js"></script>
    <link rel="stylesheet" href="https://cdn.datatables.net/1.13.3/css/jquery.dataTables.min.css">
    
    <script>
        $(document).ready(function() {
            // Initialize DataTables with custom settings for unconstrainedLeaderboardTable
            $('#unconstrainedLeaderboardTable').DataTable({
                "pageLength": -1,  // Display all rows on a single page
                "order": [[5, "desc"]]  // Sort by ImageNet-1K Top 1 accuracy in descending order
            });
            
            // Initialize DataTables for all tables
            $('#constrainedLeaderboardTable').DataTable();
            $('#unconstrainedLeaderboardTable').DataTable();
            $('#smallLeaderboardTable').DataTable();
            $('#mediumLeaderboardTable').DataTable();
            $('#largeLeaderboardTable').DataTable();
        });

        function showTable(table) {
            const constrainedTable = document.getElementById('constrainedTable');
            const unconstrainedTable = document.getElementById('unconstrainedTable');
            const constrainedTab = document.getElementById('constrainedTab');
            const unconstrainedTab = document.getElementById('unconstrainedTab');

            if (table === 'constrained') {
                constrainedTable.classList.remove('hidden');
                unconstrainedTable.classList.add('hidden');
                constrainedTab.classList.add('bg-blue-500', 'text-white');
                unconstrainedTab.classList.remove('bg-blue-500', 'text-white');
                unconstrainedTab.classList.add('bg-gray-300', 'text-black');
                showSubTable('small'); // Default to Small tab
            } else {
                constrainedTable.classList.add('hidden');
                unconstrainedTable.classList.remove('hidden');
                unconstrainedTab.classList.add('bg-blue-500', 'text-white');
                constrainedTab.classList.remove('bg-blue-500', 'text-white');
                constrainedTab.classList.add('bg-gray-300', 'text-black');
            }
        }

        function showSubTable(size) {
            const smallTable = document.getElementById('smallTable');
            const mediumTable = document.getElementById('mediumTable');
            const largeTable = document.getElementById('largeTable');
            const smallTab = document.getElementById('smallTab');
            const mediumTab = document.getElementById('mediumTab');
            const largeTab = document.getElementById('largeTab');

            smallTable.classList.add('hidden');
            mediumTable.classList.add('hidden');
            largeTable.classList.add('hidden');

            smallTab.classList.remove('bg-blue-500', 'text-white');
            mediumTab.classList.remove('bg-blue-500', 'text-white');
            largeTab.classList.remove('bg-blue-500', 'text-white');

            smallTab.classList.add('bg-gray-300', 'text-black');
            mediumTab.classList.add('bg-gray-300', 'text-black');
            largeTab.classList.add('bg-gray-300', 'text-black');

            if (size === 'small') {
                smallTable.classList.remove('hidden');
                smallTab.classList.add('bg-blue-500', 'text-white');
            } else if (size === 'medium') {
                mediumTable.classList.remove('hidden');
                mediumTab.classList.add('bg-blue-500', 'text-white');
            } else if (size === 'large') {
                largeTable.classList.remove('hidden');
                largeTab.classList.add('bg-blue-500', 'text-white');
            }
        }

    </script>

    <footer class="footer py-4 text-center animate__animated animate__fadeInUp animate__delay-1s">
        <div class="container mx-auto">
            <p>MaxCLIP - Inspiring Innovation in CLIP Optimization | &copy; 2024</p>
        </div>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const navbarToggle = document.getElementById('navbarToggle');
            const mobileMenu = document.getElementById('mobileMenu');
            
            navbarToggle.addEventListener('click', function () {
                mobileMenu.classList.toggle('show');
            });
        });
    </script>    
    
</body>
</html>